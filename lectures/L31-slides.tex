\documentclass[aspectratio=43]{beamer}

% Text packages to stop warnings
\usepackage{lmodern}
\usepackage{tikz}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ulem}
\usepackage{listings}

\usetikzlibrary{arrows,automata,shapes}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=2.5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bw} = [rectangle, draw, fill=blue!20, 
    text width=3.5em, text centered, rounded corners, minimum height=2em]

% Themes
\usetheme{Boadilla}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{navigation symbols}{}

% Suppress the navigation bar
\beamertemplatenavigationsymbolsempty

\newenvironment{changemargin}[1]{% 
  \begin{list}{}{% 
    \setlength{\topsep}{0pt}% 
    \setlength{\leftmargin}{#1}% 
    \setlength{\rightmargin}{1em}
    \setlength{\listparindent}{\parindent}% 
    \setlength{\itemindent}{\parindent}% 
    \setlength{\parsep}{\parskip}% 
  }% 
  \item[]}{\end{list}} 

\lstset{basicstyle=\scriptsize, frame=single}

\title{Lecture 22---Message Passing Interface~(MPI) \& Clusters; The Cloud}
\subtitle{ECE 459: Programming for Performance}
\date{March 27, 2014}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain]
  \titlepage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{MPI \& Clusters}
\frame{\partpage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Continuing to Switch Gears}

  \begin{changemargin}{1cm}

  So far, we've seen how to make things fast on one computer:
\begin{itemize}
\item threads;
\item compiler optimizations;
\item GPUs.
\end{itemize}
  To get a lot of bandwidth, though, you need lots of computers, \\
   \qquad (if you're lucky and the problem allows!)\\[1em]

  Today: programming for performance \\ \qquad with multiple computers via MPI.

  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Key Idea: Explicit Communication}

  \begin{changemargin}{2cm}
  Mostly we've seen shared-memory systems;\\
  \qquad complication: must manage contention.\\[1em]

  Recently, GPU programming: explicitly copy data.\\[1em]

  Message-passing: yet another paradigm.

  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is MPI?}

  \begin{changemargin}{1.5cm}

  {\bf Message Passing Interface:}

  A language-independent communation protocol\\ for parallel computers.

  \begin{itemize}
    \item Use it to run the same code on a number of \structure{nodes}\\ \qquad (different hardware
      threads; or servers in a cluster).
    \item Provides explicit message passing between nodes.
    \item Is the dominant model for high performance computing\\ \qquad (de-facto~standard).
  \end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{High Level View of MPI}

  \begin{changemargin}{2cm}

    MPI is a type of SPMD (single process, multiple data).\\[2em]

    Idea: have multiple instances of the same program, \\ all working on
      different data.\\[0.5em]

    The program could be running on the same machine, \\ or a cluster of machines.\\[0.5em]

    MPI facilitates communication of data between processes.
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{MPI Functions}
  \begin{changemargin}{1cm}

  \begin{lstlisting}
// Initialize MPI
int MPI_Init(int *argc, char **argv)

// Determine number of processes within a communicator
int MPI_Comm_size(MPI_Comm comm, int *size)

// Determine processor rank within a communicator
int MPI_Comm_rank(MPI_Comm comm, int *rank)

// Exit MPI (must be called last by all processors)
int MPI_Finalize()

// Send a message
int MPI_Send (void *buf,int count, MPI_Datatype datatype,
              int dest, int tag, MPI_Comm comm)

// Receive a message
int MPI_Recv (void *buf,int count, MPI_Datatype datatype,
              int source, int tag, MPI_Comm comm,
              MPI_Status *status)
    
  \end{lstlisting}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{MPI Function Notes}
  \begin{changemargin}{1cm}

  \begin{itemize}
    \item {\tt MPI\_Comm}: a \structure{communicator}, \\
      \qquad often {\tt MPI\_COMM\_WORLD} for global channel.
    \item {\tt MPI\_Datatype}: just an enum, e.g. {\tt MPI\_FLOAT\_INT}, etc.\\[1em]


    \item {\tt dest}/{\tt source}: ``rank'' of the process (in a communicator) \\
      to send a
      message to/receive a message from;\\
        \qquad you may use {\tt MPI\_ANY\_SOURCE} in {\tt MPI\_Recv}.\\[1em]

    \item Both {\tt MPI\_Send} and {\tt MPI\_Recv} are blocking calls---\\
      \qquad see {\tt man MPI\_Send} or {\tt man MPI\_Recv} for more details.

    \item The {\tt tag} allows you to organize your messages, \\ \qquad so you can
      filter all but a specific tag.
  \end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Hello, World in MPI}
  \begin{changemargin}{1cm}
As with OpenCL kernels:\\
\quad first, figure out what ``current'' process is supposed to compute.\\

{\small
\begin{lstlisting}[language=C]
// http://www.dartmouth.edu/~rc/classes/intro_mpi/
#include <stdio.h>
#include <mpi.h>

int main (int argc, char * argv[])
{
  int rank, size;

  /* start MPI */
  MPI_Init (&argc, &argv);	
  /* get current process id */
  MPI_Comm_rank (MPI_COMM_WORLD, &rank);	
  /* get number of processes */
  MPI_Comm_size (MPI_COMM_WORLD, &size);	
  printf("Hello world from process %d of %d\n", rank, size);
  MPI_Finalize();
  return 0;
}
\end{lstlisting}
}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Longer MPI Example (from Wikipedia)}
  \begin{changemargin}{1cm}

  Here's a common example:

  \begin{itemize}
    \item The ``master'' (rank 0) process creates some strings \\ \qquad and sends them
      to the worker processes.
    \item The worker processes modify their string \\ \qquad and send it back to the master.
  \end{itemize}

  Source:\\
  {\small \url{http://en.wikipedia.org/wiki/Message_Passing_Interface}}.

  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Example Code (1)}
  \begin{changemargin}{1cm}

  \begin{lstlisting}
/*
  "Hello World" MPI Test Program
 */
 #include <mpi.h>
 #include <stdio.h>
 #include <string.h>
 
 #define BUFSIZE 128
 #define TAG 0
 
 int main(int argc, char *argv[])
 {
   char idstr[32];
   char buff[BUFSIZE];
   int numprocs;
   int myid;
   int i;
   MPI_Status stat;
  \end{lstlisting}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Example Code (2)}
  \begin{changemargin}{1cm}

  \begin{lstlisting}
   /* all MPI programs start with MPI_Init; all 'N'
    * processes exist thereafter
    */
   MPI_Init(&argc,&argv); 

   /* find out how big the SPMD world is */
   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);

   /* and this processes' rank is what? */
   MPI_Comm_rank(MPI_COMM_WORLD, &myid);
 
   /* At this point, all programs are running equivalently;
    * the rank distinguishes the roles of the programs in
    * the SPMD model, with rank 0 often used specially...
    */
  \end{lstlisting}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Example Code (3)}
  \begin{changemargin}{1cm}

  \begin{lstlisting}
   if(myid == 0)
   {
     printf("%d: We have %d processors\n", myid, numprocs);
     for(i=1; i<numprocs; i++)
     {
       sprintf(buff, "Hello %d! ", i);
       MPI_Send(buff, BUFSIZE, MPI_CHAR, i, TAG,
                MPI_COMM_WORLD);
     }
     for(i=1; i<numprocs; i++)
     {
       MPI_Recv(buff, BUFSIZE, MPI_CHAR, i, TAG,
                MPI_COMM_WORLD, &stat);
       printf("%d: %s\n", myid, buff);
     }
   }
  \end{lstlisting}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Example Code (4)}
  \begin{changemargin}{1cm}

  \begin{lstlisting}
   else
   {
     /* receive from rank 0: */
     MPI_Recv(buff, BUFSIZE, MPI_CHAR, 0, TAG,
              MPI_COMM_WORLD, &stat);
     sprintf(idstr, "Processor %d ", myid);
     strncat(buff, idstr, BUFSIZE-1);
     strncat(buff, "reporting for duty", BUFSIZE-1);
     /* send to rank 0: */
     MPI_Send(buff, BUFSIZE, MPI_CHAR, 0, TAG,
              MPI_COMM_WORLD);
   }
 
   /* MPI Programs end with MPI Finalize; this is a weak
    * synchronization point.
    */
   MPI_Finalize();
   return 0;
 }
  \end{lstlisting}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Compiling with OpenMPI}
  \begin{changemargin}{1cm}

  \begin{lstlisting}
// Wrappers for gcc (C/C++)
mpicc
mpicxx

// Compiler Flags
OMPI_MPICC_CFLAGS
OMPI_MPICXX_CXXFLAGS

// Linker Flags
OMPI_MPICC_LDFLAGS
OMPI_MPICXX_LDFLAGS
  \end{lstlisting}

OpenMPI does not recommend that you set the flags yourself. \\ To see them, try:

  \begin{lstlisting}
# Show the flags necessary to compile MPI C applications
shell$ mpicc --showme:compile

# Show the flags necessary to link MPI C applications
shell$ mpicc --showme:link
  \end{lstlisting}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Compiling and Running}

  \begin{changemargin}{1.5cm}

  \begin{lstlisting}
mpirun -np <num_processors> <program>
mpiexec -np <num_processors> <program> # a synonym
  \end{lstlisting}

  
   Starts {\tt num\_processors} instances of the program using MPI.

  \begin{lstlisting}
jon@riker examples master % mpicc hello_mpi.c 
jon@riker examples master % mpirun -np 8 a.out
0: We have 8 processors
0: Hello 1! Processor 1 reporting for duty
0: Hello 2! Processor 2 reporting for duty
0: Hello 3! Processor 3 reporting for duty
0: Hello 4! Processor 4 reporting for duty
0: Hello 5! Processor 5 reporting for duty
0: Hello 6! Processor 6 reporting for duty
0: Hello 7! Processor 7 reporting for duty
  \end{lstlisting}

  \begin{itemize}
    \item By default, MPI uses the lowest-latency communication resource available; shared 
      memory, in this case.
  \end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{MPI Matrix Multiplication Example}

 Highlights of:
   {\scriptsize \url{http://www.nccs.gov/wp-content/training/mpi-examples/C/matmul.c}}.\\
  \begin{changemargin}{2cm}

  To compute the matrix product $AB$:
\begin{enumerate}
\item Initialize MPI.
\item If the current process is the master task (task id 0):
\begin{enumerate}
\item Initialize matrices.
\item Send work to each worker task: \\
row number (offset); number of rows;
row contents from $A$; complete contents of matrix $B$.

{\scriptsize
\begin{verbatim}
MPI_Send(&a[offset][0], rows*NCA, MPI_DOUBLE, dest, 
         mtype, MPI_COMM_WORLD);
\end{verbatim}
}

\item Wait for results from all worker tasks (\verb+MPI_Recv+).
\item Print results.
\end{enumerate}
\item For all other tasks:
\begin{enumerate}
\item Receive offset, number of rows, partial matrix $A$, and complete matrix $B$, using {\tt MPI\_Recv}:

{\scriptsize
\begin{verbatim}
MPI_Recv(&offset, 1, MPI_INT, MASTER, 
         mtype, MPI_COMM_WORLD, &status);
\end{verbatim}
}
\item Do the computation.
\item Send the results back to the sender.
\end{enumerate}
\end{enumerate}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Other Things MPI Can Do}
  
  \begin{changemargin}{2cm}

    We can use nodes on a network (by using a {\tt hostfile}).\\[1em]
    We can even use MPMD:
      \begin{itemize}
        \item {\it multiple processes, multiple data}
      \end{itemize}~\\

  \begin{lstlisting}
% mpirun -np 2 a.out : -np 2 b.out
  \end{lstlisting}

  This launches a single parallel application.

  \begin{itemize}
    \item All in the same {\tt MPI\_COMM\_WORLD}; but
    \item Ranks 0 and 1 are instances of {\tt a.out}, and
    \item Ranks 2 and 3 are instances of {\tt b.out}.
  \end{itemize}~\\

  You could also use the {\tt --app} flag with an appfile \\ instead of typing out
  everything.
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Performance Considerations}

  \begin{changemargin}{1cm}

 Your bottleneck for performance here is message-passing.\\[1em]
 Keep the communication to a minimum!\\[1em]
 In general, the more machines/farther apart they are, the slower the communication.\\[1em]

Each step from multicore
machines to GPU programming to MPI triggers an
order-of-magnitude decrease in communication bandwidth and  similar
increase in latency.
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Cloud Computing}
\frame{\partpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Using a Cluster}

  \begin{changemargin}{1cm}
    Historically:
\begin{itemize}
  \item find \$\$\$;
  \item buy and maintain pile of expensive machines.
\end{itemize}

  Not anymore! \\[1em]

  We'll talk about Amazon's Elastic Compute Cloud (EC2)\\ and
  principles behind it.
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Evolution of servers}

  \begin{changemargin}{1cm}

You want a server on the Internet.
\begin{itemize}
\item 
  Once upon a time: had to get a physical machine hosted (e.g. in a rack).\\
  Or, live with inferior shared hosting.\\[1em]
\item Virtualization: pay for part of a
  machine on that rack.  \\
  A win: you're usually not maxing out a computer, and you'd
  be perfectly happy to share it with others, as long as there are
  good security guarantees. All users can get root access.\\[1em]
\item Clouds enable you to add more machines on-demand. \\ Instead of
  having just one virtual server, spin up dozens (or
  thousands) of server images when you need more compute
  capacity. \\

  Servers typically share persistent storage, also in
  the cloud. 
\end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Paying for Computes}

  \begin{changemargin}{1cm}
Cloud computing:
\begin{itemize}
\item pay according to the number of machines, or
instances, that you've started up.
\end{itemize}
~\\[1em]

Providers offer different instance
sizes;\\
\qquad sizes vary according to the \\ \qquad number of cores, local
storage, and memory.\\[1em]

Some instances even have GPUs!
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Launching Instances}

  \begin{changemargin}{2cm}
Need more computes? Launch an instance!\\[1em]

Input: Virtual Machine image.\\[1em]

Mechanics: use a command-line or web-based tool.\\[1em]

New instance gets an IP address and is network-accessible. \\
You have full root access to that instance.
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What to Launch?}

  \begin{changemargin}{2cm}
Amazon provides public images:
\begin{itemize}
\item different Linux distributions;
\item Windows Server; and
\item OpenSolaris (maybe not anymore?). 
\end{itemize}

You can build an image which contains software you
want, including Hadoop and OpenMPI.
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Cleanup}

  \begin{changemargin}{1cm}
Presumably you don't want to pay forever for your instances.\\[1em]

When you're done with an instance:
\begin{itemize}
\item shut it down, stop paying for it.
\end{itemize}

All data on instance goes away.
  \end{changemargin}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data Storage}
  \begin{changemargin}{2cm}
To keep persistent results:
\begin{itemize}
\item mount a storage device,
also on the cloud (e.g. Amazon Elastic Block Storage); or, 
\item connect to a database on a persistent server (e.g. Amazon SimpleDB or
Relational Database Service); or, 
\item you can store files on the Web (e.g. Amazon S3). 
\end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Summary}

  \begin{changemargin}{1cm}

  \begin{itemize}
    \item MPI is a powerful tool for highly parallel computing across multiple
      machines.\\[1em]
    \item MPI Programming is similar to a more powerful version of {\tt fork/join}; 
but you have to manage communication more explicitly.\\[1em]
    \item Saw cloud computing basics.
  \end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
